{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14033cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a typed DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": pd.Series([1, 2, 3], dtype=\"int64\"),\n",
    "        \"name\": pd.Series([\"Alice\", \"Bob\", \"Carol\"], dtype=\"string\"),\n",
    "        \"age\": pd.Series([30, 25, 27], dtype=\"int32\"),\n",
    "        \"salary\": pd.Series([50000.0, 60000.0, 75000.0], dtype=\"float64\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Convert to Parquet\n",
    "df.to_parquet(\"output.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee63f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                 int64\n",
      "name      string[python]\n",
      "age                int32\n",
      "salary           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_parquet(\"output.parquet\")\n",
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67905402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "   session_id  name  age   salary\n",
      "1           2   Bob   25  60000.0\n",
      "3           2   Max   55  89000.0\n",
      "4           2  John   60  90000.0\n",
      "   session_id   name  age   salary\n",
      "0           1  Alice   30  50000.0\n",
      "2           1  Carol   27  75000.0\n",
      "4\n",
      "   session_id  name  age   salary  window_id\n",
      "1           2   Bob   25  60000.0         20\n",
      "3           2   Max   55  89000.0         20\n",
      "4           2  John   60  90000.0         20\n",
      "   session_id   name  age   salary  window_id\n",
      "0           1  Alice   30  50000.0         10\n",
      "2           1  Carol   27  75000.0         10\n",
      "   session_id  name  age   salary  window_id\n",
      "1           2   Bob   25  60000.0         21\n",
      "3           2   Max   55  89000.0         21\n",
      "4           2  John   60  90000.0         21\n",
      "   session_id   name  age   salary  window_id\n",
      "0           1  Alice   30  50000.0         11\n",
      "2           1  Carol   27  75000.0         11\n",
      "   session_id  name  age   salary  window_id\n",
      "0           2   Bob   25  60000.0         21\n",
      "1           2   Max   55  89000.0         21\n",
      "2           2  John   60  90000.0         21\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Step 1: Create and save initial Parquet\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"session_id\": pd.Series([1, 2, 1, 2, 2], dtype=\"int64\"),\n",
    "        \"name\": pd.Series([\"Alice\", \"Bob\", \"Carol\", \"Max\", \"John\"], dtype=\"string\"),\n",
    "        \"age\": pd.Series([30, 25, 27, 55, 60], dtype=\"int32\"),\n",
    "        \"salary\": pd.Series([50000.0, 60000.0, 75000.0, 89000, 90000], dtype=\"float64\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "ddf = dd.from_pandas(df)\n",
    "ddf = ddf.shuffle(\"session_id\", npartitions=df[\"session_id\"].nunique())\n",
    "# ddf = ddf.set_index(\"session_id\", npartitions=df[\"session_id\"].nunique())\n",
    "print(ddf.npartitions)\n",
    "\n",
    "ddf.to_parquet(\"input_dir/\", engine=\"pyarrow\")\n",
    "\n",
    "# Step 2: Load with Dask\n",
    "ddf = dd.read_parquet(\"input_dir/\", engine=\"pyarrow\")\n",
    "\n",
    "for i in range(ddf.npartitions):\n",
    "    print(ddf.get_partition(i).compute())\n",
    "\n",
    "# print(len(ddf.columns), len(ddf))\n",
    "# print(df1.shape, df2.shape)\n",
    "\n",
    "\n",
    "# Step 3: Fake windowing function\n",
    "def create_windows(df):\n",
    "    rows = []\n",
    "    for i in range(2):\n",
    "        copy = df.copy()\n",
    "        # create unique window id per window and session\n",
    "        copy[\"window_id\"] = copy[\"session_id\"] * 10 + i\n",
    "        # copy[\"partition_key\"] = copy[\"id\"] * 10 + i  # Unique per window\n",
    "        rows.append(copy)\n",
    "    return pd.concat(rows)\n",
    "\n",
    "\n",
    "# Step 4: Apply to partitions\n",
    "windowed = ddf.map_partitions(create_windows)\n",
    "windowed = windowed.shuffle(\n",
    "    \"window_id\", npartitions=windowed[\"window_id\"].nunique().compute()\n",
    ")\n",
    "\n",
    "print(windowed.npartitions)\n",
    "\n",
    "windowed.to_parquet(\"output_windows/\", engine=\"pyarrow\", write_index=False)\n",
    "\n",
    "for i in range(windowed.npartitions):\n",
    "    print(windowed.get_partition(i).compute())\n",
    "\n",
    "\n",
    "# load only partition 3 from disk\n",
    "windowed = dd.read_parquet(\"output_windows/part.2.parquet\", engine=\"pyarrow\")\n",
    "print(windowed.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca9ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files: ('session_1.parquet', 'session_2.parquet')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask\n",
    "\n",
    "# Step 1: Create and save initial Parquet\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"session_id\": pd.Series([1, 2, 1, 2, 2], dtype=\"int64\"),\n",
    "        \"name\": pd.Series([\"Alice\", \"Bob\", \"Carol\", \"Max\", \"John\"], dtype=\"string\"),\n",
    "        \"age\": pd.Series([30, 25, 27, 55, 60], dtype=\"int32\"),\n",
    "        \"salary\": pd.Series([50000.0, 60000.0, 75000.0, 89000, 90000], dtype=\"float64\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "ddf = dd.from_pandas(df)\n",
    "\n",
    "\n",
    "# Define a function to save a single group as Parquet\n",
    "@dask.delayed\n",
    "def save_group(session_id):\n",
    "    filename = f\"session_{session_id}.parquet\"\n",
    "    group_df = ddf[ddf[\"session_id\"] == sid].compute()\n",
    "    group_df.to_parquet(filename)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Compute the unique session_ids\n",
    "session_ids = ddf[\"session_id\"].unique().compute()\n",
    "\n",
    "# Collect delayed tasks for each session_id group\n",
    "tasks = []\n",
    "for sid in session_ids:\n",
    "    # convert group to pandas df\n",
    "    tasks.append(save_group(sid))\n",
    "\n",
    "# Trigger the actual saving in parallel\n",
    "saved_files = dask.compute(*tasks)\n",
    "\n",
    "print(\"Saved files:\", saved_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
